# æ•°å€¼ç§¯åˆ†è¯¾ç¨‹ç¼–ç¨‹å®éªŒ

æ¬¢è¿æ¥åˆ°æœ¬æ¬¡å®éªŒçš„ä»£ç ä»“åº“ï¼æœ¬æ¬¡å®éªŒæ¶µç›–è¯¾ç¨‹ç¬¬6-8ç« çš„å†…å®¹ï¼Œå¯¹åº”çš„ä¸»ç¨‹åºæ–‡ä»¶ä¸º `sovle_le.py`å’Œ `flash_atten`æ–‡ä»¶å¤¹ã€‚å¸Œæœ›å¤§å®¶åœ¨æ¢ç´¢æ•°å€¼åˆ†æçš„è¿‡ç¨‹ä¸­ï¼Œæ—¢èƒ½æ”¶è·çŸ¥è¯†ï¼Œä¹Ÿèƒ½äº«å—ç¼–ç¨‹çš„ä¹è¶£ ğŸ˜Šã€‚

---

## å®éªŒç¯å¢ƒ

1. **Python 3.9+**  
   æ¨èä½¿ç”¨ Python 3.9 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚

2. **PyTorch 2.0+**  
   å»ºè®®å®‰è£… PyTorch 2.0 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚

3. **CUDA Toolkit**ï¼ˆå¯é€‰ï¼‰

   CUDA ç‰ˆæœ¬éœ€ä¸ PyTorch å’Œ FlashAttention åº“å…¼å®¹ï¼Œæ¨èä½¿ç”¨ CUDA 11.x æˆ– 12.xã€‚

4. **FlashAttention å®˜æ–¹åº“**  
   æ¨èé€šè¿‡ PyPI å®‰è£… `flash-attn` åŒ…ã€‚éœ€æ³¨æ„å®‰è£…è¯¥åº“å¯èƒ½éœ€è¦ç‰¹å®šçš„ç¼–è¯‘ç¯å¢ƒåŠ GPU æ¶æ„ï¼ˆå¦‚ NVIDIA Ampere ç³»åˆ—ï¼‰æ”¯æŒã€‚è‹¥å®‰è£…é‡åˆ°å›°éš¾ï¼Œå¯å°†ç›¸å…³å¯¹æ¯”ä½œä¸ºé€‰åšé¡¹æˆ–è¿›è¡Œç†è®ºåˆ†æã€‚

5.  **NumPy**
6.  **SciPy**

---

## ç¯å¢ƒæ­å»º
1. å¦‚æœä½ ä½¿ç”¨å­¦é™¢å®éªŒå®¤è®¡ç®—æœºæˆ–è€…æ²¡æœ‰é…ç½®condaå’Œcudaï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…condaå’Œcuda,å¦åˆ™è·³è¿‡è¿™ä¸€æ­¥ã€‚

é¦–å…ˆè¿›å…¥Ubantu1ç³»ç»Ÿ
æ‰“å¼€ç»ˆç«¯ï¼Œä¾æ¬¡è¾“å…¥ä»¥ä¸‹å‘½ä»¤
```bash
cd /home/nju/
# å­¦é™¢æœåŠ¡å™¨æœ¬åœ°æœ‰miniconda.shï¼Œå¦‚æœæ²¡æœ‰è¯·æ‰§è¡Œwget -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.shé¦–å…ˆä¸‹è½½
bash Miniconda3-latest-Linux-x86_64.sh  # ç„¶åä¸æ–­enterï¼Œè¾“å…¥yesï¼Œå®‰è£…å®Œæˆåé‡å¼€ä¸€ä¸ªç»ˆç«¯
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
chmod 755 cuda_11.8.0_520.61.05_linux.run
mkdir cuda
sh cuda_11.8.0_520.61.05_linux.run
```
è¿™æ—¶è¿›å…¥cudaå®‰è£…é¡µé¢
- é€‰æ‹©continue
- è¾“å…¥accept
- å»é™¤æ‰€æœ‰çš„xï¼Œåªä¿ç•™CUDA Toolkit
- è¿›å…¥Optionsï¼Œç„¶åé€‰æ‹©Toolkit Options ï¼Œå»æ‰æ‰€æœ‰çš„X
- å†é€‰æ‹©Change Toolkit Install Pathï¼Œè¾“å…¥/home/nju/cuda
- å›åˆ°Optionsï¼Œé€‰æ‹©Library install pathï¼Œä¿®æ”¹ä¸º/home/nju/cuda
- å›åˆ°ä¸»ç•Œé¢ï¼Œé€‰æ‹©Install

å®‰è£…å®Œæˆåè¾“å…¥ä»¥ä¸‹å‘½ä»¤
```bash
echo 'export PATH=/home/nju/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/home/nju/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc         #å®‰è£…å®Œæˆ
nvcc -V                  #è¾“å…¥æ­¤å‘½ä»¤éªŒè¯å®‰è£…æˆåŠŸ

```

2.é…ç½®è™šæ‹Ÿç¯å¢ƒ
```bash
conda create -n Numerical_Analysis_02 python==3.10.14  # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda activate Numerical_Analysis_02                  # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia   # å®‰è£…pytorch
git clone https://git.nju.edu.cn/mq-yuan/numerical_analysis_2025.git --branch exp2
cd numerical_analysis_2025
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple                # å®‰è£…ä¾èµ–
```
ä»[njuäº‘ç›˜](https://box.nju.edu.cn/d/283b5e195ea349cea037/)ä¸‹è½½å¯¹åº”whlæ–‡ä»¶ï¼Œæˆ–è€…å…¶ä»–ç‰ˆæœ¬åœ¨[flash-attn-linux](https://github.com/Dao-AILab/flash-attention/releases), [flash-attn-window](https://github.com/kingbri1/flash-attention/releases)ä¸‹è½½ï¼Œç„¶ååœ¨ç»ˆç«¯æ‰§è¡Œ
```bash
pip install flash_attn-2.7.3+cu11torch2.1cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
```

---
## å®éªŒæ¡†æ¶
æœ¬å®éªŒçš„ä»£ç æ¡†æ¶å·²ç»æ­å»ºå®Œæˆï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
- `solve_le.py`ï¼šå®ç°é«˜æ–¯æ¶ˆå»æ³•å’Œé«˜æ–¯-èµ›å¾·å°”è¿­ä»£æ³•æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„çš„ä»£ç ã€‚
- `flash_atten/`ï¼šå®ç° FlashAttention çš„æ ¸å¿ƒä»£ç ã€‚
  - `main_test.py`ï¼šæµ‹è¯•ä»£ç ï¼ŒåŒ…å«å¯¹æ¯”å®éªŒçš„å®ç°ã€‚
  - `atten_model.py`ï¼šå®ç°å¯¹æ¯”å®éªŒä¸­æ‰€éœ€çš„æ¨¡å‹ã€‚
  - `custom_flashaten.py`ï¼šéœ€è¦å®ç°çš„ FlashAttention ä»£ç ã€‚
  - `flash_atten_backward.py`ï¼šéœ€è¦å®ç° FlashAttention çš„åå‘ä¼ æ’­ã€‚
- `README.md`ï¼šå®éªŒè¯´æ˜æ–‡æ¡£ã€‚
- `requirements.txt`ï¼šä¾èµ–åŒ…åˆ—è¡¨ã€‚

## ä¼ªä»£ç 
ä¸‹é¢ç»™å‡ºäº† FlashAttention çš„ä¼ªä»£ç ä¾›å‚è€ƒã€‚

   ***Forwoard***
   
   **Inputs:** $Q, K, V$, Tile sizes $T_q, T_k$. (Shapes: $Q \in \mathbb{R}^{B \times H \times N_q \times D_h}$, etc.). Let $s = 1/\sqrt{D_h}$ be the softmax scale.

   **Initialize:**
   $O_{full} = \mathbf{0}$ (tensor of zeros, shape $B \times H \times N_q \times D_h$)

   For each query tile $Q_i$ (from $Q$, with length up to $T_q$):
   1.  $S_i^{full} = \mathbf{0}$ (tensor of zeros, shape $B \times H \times \text{length}(Q_i) \times N_{kv}$)

   2.  For each key tile $K_j$ (from $K$, with length up to $T_k$):

         a.  **Calculate Partial Scores**: $S_{ij} = (Q_i \cdot K_j^T) \times s$
         
         b.  **Accumulate Scores**: Store $S_{ij}$ into the corresponding columns of $S_i^{full}$.
         (e.g., if $K_j$ covers columns $k_{start}$ to $k_{end}$ of $K$, then $S_i^{full}[:,:,:, k_{start}:k_{end}] = S_{ij}$)

   3.  $P_i = \text{softmax}(S_i^{full}, \text{dim}=-1)$
   4.  $O_i = P_i \cdot V$ (Matrix multiply $P_i$ with the full $V$ tensor)
   5.  Place $O_i$ into the corresponding rows of $O_{full}$.

   **Return:** $O_{full}$
   
   ---
   ***Backward***
   
   **Inputs:** $dO, Q, K, V, T_q, T_k$. (Let $s = 1/\sqrt{D_h}$ be softmax scale. Shapes: $dO, Q \in \mathbb{R}^{B \times H \times N_q \times D_h}$; $K, V \in \mathbb{R}^{B \times H \times N_{kv} \times D_h}$)

   **Initialize:** $dQ = \mathbf{0}, dK = \mathbf{0}, dV = \mathbf{0}$ (zero tensors with shapes of $Q, K, V$ respectively).

   For each query tile $Q_i$ (from $Q$, with length $N_{q\_curr}$ up to $T_q$) and corresponding $dO_i$ (from $dO$):

   6.  **Recompute attention scores $S_i^{full}$ and probabilities $P_i$ for $Q_i$ against all $K$**:
      
         a.  $S_i^{full} = \mathbf{0}$ (tensor of zeros, shape $B \times H \times N_{q\_curr} \times N_{kv}$)
         
         b.  For each key tile $K_j$ (from $K$, with length up to $T_k$):

         i.  $S_{ij} = (Q_i \cdot K_j^T) \cdot s$
      
         ii. Store $S_{ij}$ into the corresponding columns of $S_i^{full}$.
         (e.g., if $K_j$ covers columns $k_{start}$ to $k_{end}$ of $K$, then $S_i^{full}[:,:,:, k_{start}:k_{end}] = S_{ij}$)
         
         c.  $P_i = \text{softmax}(S_i^{full}, \text{dim}=-1)$

   7.  **Compute and Accumulate Gradients**:
      
         a.  $dV = dV + (P_i^T \cdot dO_i)$
         
         b.  $dP_i = dO_i \cdot V^T$
         
         c.  $dS_i = P_i \odot (dP_i - \sum_{\text{last dim}}(dP_i \odot P_i, \text{keepdim=True}))$
            ($\odot$ denotes element-wise product)
         
         d.  $dQ_i^{\text{block}} = (dS_i \cdot K) \cdot s$. Place $dQ_i^{\text{block}}$ into the rows of $dQ$ corresponding to $Q_i$.
         
         e.  $dK = dK + (dS_i^T \cdot Q_i) \cdot s$

   **Return:** $dQ, dK, dV$

---

æœ€ç»ˆï¼Œä½ éœ€è¦æäº¤æœ¬ä»“åº“ä»£ç çš„æ‰“åŒ…æ–‡ä»¶ä»¥åŠå®éªŒæŠ¥å‘Šï¼Œæäº¤é“¾æ¥è§è¯¾ç¨‹ä¸»é¡µã€‚åŠ æ²¹ï¼ğŸš€
